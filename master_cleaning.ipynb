{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOp21USFuwJMcJ93AeHk7N/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PolinaLinaNechaeva/lfs-crm-analytics/blob/main/master_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xoeQqhcdF5X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# 1. Read spend file\n",
        "data_spend = pd.read_excel(\"Spend (Done).xlsx\")\n",
        "print(data_spend.head(5))\n",
        "print(data_spend.tail(5))\n",
        "\n",
        "data_spend.info()\n",
        "\n",
        "# bring Date to unified format (dd.mm.yyyy), NaT → 31.12.2099\n",
        "if \"Date\" in data_spend.columns:\n",
        "    parsed = pd.to_datetime(data_spend[\"Date\"], errors=\"coerce\", dayfirst=True)\n",
        "\n",
        "    if parsed.isna().all():\n",
        "        # try another format if everything failed\n",
        "        parsed = pd.to_datetime(\n",
        "            data_spend[\"Date\"],\n",
        "            errors=\"coerce\",\n",
        "            format=\"%Y-%m-%d %H:%M:%S\",\n",
        "        )\n",
        "\n",
        "    parsed = parsed.fillna(pd.to_datetime(\"2099-12-31\"))\n",
        "    data_spend[\"Date\"] = parsed.dt.strftime(\"%d.%m.%Y\")\n",
        "\n",
        "# check\n",
        "print(data_spend[\"Date\"].head(10))\n",
        "print(\"NaN remaining:\", data_spend[\"Date\"].isna().sum())\n",
        "\n",
        "# percentage of missing values in each column\n",
        "nan_percent = (data_spend.isna().sum() / len(data_spend) * 100).round(2)\n",
        "print(nan_percent)\n",
        "\n",
        "data_spend[\"Campaign\"] = data_spend[\"Campaign\"].fillna(\"\")\n",
        "data_spend[\"AdGroup\"] = data_spend[\"AdGroup\"].fillna(\"\")\n",
        "data_spend[\"Ad\"] = data_spend[\"Ad\"].fillna(\"\")\n",
        "nan_percent = (data_spend.isna().sum() / len(data_spend) * 100).round(2)\n",
        "print(nan_percent)\n",
        "\n",
        "# Find and remove full duplicates\n",
        "dups_full = data_spend[data_spend.duplicated(keep=False)]\n",
        "\n",
        "# all full duplicates\n",
        "print(dups_full.head(10))\n",
        "print(dups_full.tail(10))\n",
        "\n",
        "print(f\"Full duplicates found: {dups_full.shape[0]}\")\n",
        "\n",
        "before = len(data_spend)\n",
        "# remove full duplicate rows\n",
        "data_spend = data_spend.drop_duplicates()\n",
        "after = len(data_spend)\n",
        "\n",
        "print(f\"Removed {before - after} full duplicates. Rows left: {after}\")\n",
        "\n",
        "# save cleaned dataframe\n",
        "data_spend.to_excel(\"data_spend_clean.xlsx\", index=False)\n",
        "\n",
        "print(\"File saved successfully: data_spend_clean.xlsx\")\n",
        "\n",
        "\"\"\"## Deals Cleaning\"\"\"\n",
        "\n",
        "# 1. Read deals\n",
        "data_deals = pd.read_excel(\"DWW Deals Done.xlsx\")\n",
        "print(data_deals.head(5))\n",
        "print(data_deals.tail(5))\n",
        "\n",
        "data_deals.info()\n",
        "\n",
        "# Ranges for numerical fields\n",
        "numeric_cols = data_deals.select_dtypes(include=[np.number]).columns\n",
        "# ranges for all numeric columns\n",
        "for col in numeric_cols:\n",
        "    col_min = data_deals[col].min()\n",
        "    col_max = data_deals[col].max()\n",
        "    print(f\"{col}: min = {col_min}, max = {col_max}\")\n",
        "\n",
        "print(\"Ranges for numeric fields:\")\n",
        "\n",
        "# Check free columns\n",
        "nan_percent_deals = (data_deals.isna().sum() / len(data_deals) * 100).round(2)\n",
        "print(nan_percent_deals)\n",
        "\n",
        "# Work with ID\n",
        "data_deals[\"DEALID\"] = data_deals[\"DEALID\"].astype(str)\n",
        "\n",
        "# Work with Amount\n",
        "data_deals[\"Amount\"] = pd.to_numeric(data_deals[\"Amount\"], errors=\"coerce\")\n",
        "data_deals[\"Amount\"].describe()\n",
        "\n",
        "# Work with pipeline\n",
        "data_deals[\"PIPELINE\"] = data_deals[\"PIPELINE\"].astype(str)\n",
        "\n",
        "# Work with Stage\n",
        "data_deals[\"STAGE\"] = data_deals[\"STAGE\"].astype(str)\n",
        "\n",
        "# Work with Lost Reason\n",
        "data_deals[\"Lost Reason\"] = data_deals[\"Lost Reason\"].astype(str)\n",
        "\n",
        "# Work with Created Date, Closing Date\n",
        "data_deals[\"Created Date\"] = pd.to_datetime(\n",
        "    data_deals[\"Created Date\"], errors=\"coerce\", dayfirst=True\n",
        ")\n",
        "data_deals[\"Closing Date\"] = pd.to_datetime(\n",
        "    data_deals[\"Closing Date\"], errors=\"coerce\", dayfirst=True\n",
        ")\n",
        "\n",
        "nan_percent_deals = (data_deals.isna().sum() / len(data_deals) * 100).round(2)\n",
        "print(nan_percent_deals)\n",
        "\n",
        "# Delete rows with Lost Reason = 'Duplicate'\n",
        "data_deals = data_deals[data_deals[\"Lost Reason\"] != \"Duplicate\"]\n",
        "\n",
        "# check result\n",
        "print(data_deals[\"Lost Reason\"].value_counts(dropna=False))\n",
        "\n",
        "print(f\"Rows left: {len(data_deals)}\")\n",
        "\n",
        "# Replace NaN in Closing Date with 1900-01-01\n",
        "data_deals[\"Closing Date\"] = data_deals[\"Closing Date\"].fillna(\n",
        "    pd.to_datetime(\"1900-01-01\")\n",
        ")\n",
        "\n",
        "# Replace NaN in SLA with mode\n",
        "if \"SLA\" in data_deals.columns:\n",
        "    sla_mode = data_deals[\"SLA\"].mode(dropna=True)\n",
        "    if not sla_mode.empty:\n",
        "        data_deals[\"SLA\"] = data_deals[\"SLA\"].fillna(sla_mode.iloc[0])\n",
        "\n",
        "# For numeric columns: NaN → 0\n",
        "num_cols = data_deals.select_dtypes(include=[np.number]).columns\n",
        "data_deals[num_cols] = data_deals[num_cols].fillna(0)\n",
        "\n",
        "# For string columns: NaN → 'Unknown'\n",
        "str_cols = data_deals.select_dtypes(include=[\"object\"]).columns\n",
        "data_deals[str_cols] = data_deals[str_cols].fillna(\"Unknown\")\n",
        "\n",
        "print(\"Missing values handled:\")\n",
        "print(data_deals.isna().sum().sum(), \"NaN remaining\")\n",
        "\n",
        "unique_reasons = data_deals[\"Lost Reason\"].unique()\n",
        "print(\"Unique reasons:\", unique_reasons)\n",
        "\n",
        "# create new column with grouped reasons\n",
        "mapping = {\n",
        "    \"price\": [\n",
        "        \"Cheaper provider\",\n",
        "        \"Price\",\n",
        "        \"Price/Costs\",\n",
        "        \"budget\",\n",
        "    ],\n",
        "    \"internal\": [\n",
        "        \"Internal HR\",\n",
        "        \"Internal Recruiter\",\n",
        "        \"Inhouse HR\",\n",
        "        \"Inhouse recruiting\",\n",
        "        \"Internal restructuring\",\n",
        "    ],\n",
        "    \"no decision\": [\n",
        "        \"No decision\",\n",
        "        \"Not now\",\n",
        "        \"Not the right time\",\n",
        "        \"Not ready\",\n",
        "    ],\n",
        "    \"other\": [\n",
        "        \"Other reasons\",\n",
        "        \"Unknown\",\n",
        "        \"Refer to competitor\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "def categorize_reason(reason: str) -> str:\n",
        "    reason_lower = str(reason).lower()\n",
        "    for category, keywords in mapping.items():\n",
        "        for kw in keywords:\n",
        "            if kw.lower() in reason_lower:\n",
        "                return category\n",
        "    return \"other\"\n",
        "\n",
        "\n",
        "data_deals[\"Lost Reason Grouped\"] = data_deals[\"Lost Reason\"].apply(\n",
        "    categorize_reason\n",
        ")\n",
        "\n",
        "# check distribution\n",
        "print(data_deals[\"Lost Reason Grouped\"].value_counts(dropna=False))\n",
        "\n",
        "# search for full duplicates\n",
        "dups_deals = data_deals[data_deals.duplicated(keep=False)]\n",
        "print(dups_deals.head(10))\n",
        "\n",
        "print(f\"Total rows in table: {len(data_deals)}\")\n",
        "print(\n",
        "    f\"Number of rows that are part of full duplicates: {len(dups_deals)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Number of unique full duplicates: {dups_deals.duplicated().sum()}\"\n",
        ")\n",
        "\n",
        "# remove duplicates and keep dataframe in variable\n",
        "data_deals_dedup = data_deals.drop_duplicates()\n",
        "\n",
        "# save separately\n",
        "data_deals_dedup.to_excel(\"DWW Deals clean.xlsx\", index=False)\n",
        "\n",
        "# save cleaned dataframe\n",
        "data_deals.to_excel(\"data_deals_clean.xlsx\", index=False)\n",
        "\n",
        "print(\"File saved successfully: data_deals_clean.xlsx\")\n",
        "\n",
        "\"\"\"## Calls Cleaning\"\"\"\n",
        "\n",
        "data_calls = pd.read_excel(\"DWW Calls Done.xlsx\")\n",
        "print(data_calls.head())\n",
        "print(data_calls.info())\n",
        "\n",
        "# Fix scientific notation in Duration if needed\n",
        "def normalize_duration(x):\n",
        "    \"\"\"\n",
        "    Normalize call duration values that might be in scientific notation or as strings.\n",
        "    \"\"\"\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    if isinstance(x, (int, float)):\n",
        "        return x\n",
        "    if isinstance(x, str):\n",
        "        # expand exponent form\n",
        "        if \"e\" in x.lower():\n",
        "            try:\n",
        "                return float(x)\n",
        "            except ValueError:\n",
        "                pass\n",
        "        x = x.replace(\"sec\", \"\").replace(\"s\", \"\").strip()\n",
        "        x = x.replace(\",\", \".\")\n",
        "        x = x.replace(\" \", \"\")\n",
        "        x = x.replace(\".\", \"\")  # remove dots\n",
        "        # keep only one prefix\n",
        "        if x.startswith(\"00:\"):\n",
        "            x = x[3:]\n",
        "        try:\n",
        "            return float(x)\n",
        "        except ValueError:\n",
        "            return np.nan\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "if \"Duration\" in data_calls.columns:\n",
        "    data_calls[\"Duration_norm\"] = data_calls[\"Duration\"].apply(normalize_duration)\n",
        "\n",
        "    print(data_calls[[\"Duration\", \"Duration_norm\"]].head(20))\n",
        "\n",
        "    # drop original column\n",
        "    data_calls = data_calls.drop(columns=[\"Duration\"])\n",
        "    data_calls = data_calls.rename(columns={\"Duration_norm\": \"Duration\"})\n",
        "\n",
        "print(data_calls.info())\n",
        "\n",
        "# Convert Call Start Time to datetime\n",
        "data_calls[\"Call Start Time\"] = pd.to_datetime(\n",
        "    data_calls[\"Call Start Time\"], errors=\"coerce\"\n",
        ")\n",
        "\n",
        "# create date-only column\n",
        "data_calls[\"Call Date\"] = data_calls[\"Call Start Time\"].dt.date\n",
        "\n",
        "# replace Call Start Time with time only\n",
        "data_calls[\"Call Time\"] = data_calls[\"Call Start Time\"].dt.time\n",
        "\n",
        "# check\n",
        "print(data_calls[[\"Call Start Time\", \"Call Date\", \"Call Time\"]].head(10))\n",
        "\n",
        "# Fill missing CONTACTID by mode per owner\n",
        "if \"CONTACTID\" in data_calls.columns and \"OWNER\" in data_calls.columns:\n",
        "    # 2) fill CONTACTID with mode per call owner\n",
        "    mode_per_owner = (\n",
        "        data_calls.groupby(\"OWNER\")[\"CONTACTID\"]\n",
        "        .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "    )\n",
        "    data_calls[\"CONTACTID\"] = data_calls.apply(\n",
        "        lambda row: mode_per_owner[row[\"OWNER\"]]\n",
        "        if pd.isna(row[\"CONTACTID\"]) and row[\"OWNER\"] in mode_per_owner\n",
        "        else row[\"CONTACTID\"],\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "# 3) fill duration with mode per call owner\n",
        "if \"Duration\" in data_calls.columns and \"OWNER\" in data_calls.columns:\n",
        "    duration_mode_per_owner = (\n",
        "        data_calls.groupby(\"OWNER\")[\"Duration\"]\n",
        "        .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "    )\n",
        "    data_calls[\"Duration\"] = data_calls.apply(\n",
        "        lambda row: duration_mode_per_owner[row[\"OWNER\"]]\n",
        "        if pd.isna(row[\"Duration\"]) and row[\"OWNER\"] in duration_mode_per_owner\n",
        "        else row[\"Duration\"],\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "# Unique statuses\n",
        "unique_statuses = data_calls[\"Call Status\"].unique()\n",
        "print(\"Unique statuses:\", unique_statuses)\n",
        "\n",
        "# Keep only Completed calls\n",
        "data_calls = data_calls[data_calls[\"Call Status\"] == \"Completed\"]\n",
        "\n",
        "print(f\"Rows left: {len(data_calls)}\")\n",
        "\n",
        "# analyze call duration\n",
        "sns.boxplot(x=data_calls[\"Duration\"])\n",
        "plt.title(\"Call duration boxplot (seconds)\")\n",
        "plt.xlabel(\"Duration (sec)\")\n",
        "plt.show()\n",
        "\n",
        "# number of full duplicates (all columns)\n",
        "dups_calls_full = data_calls[data_calls.duplicated(keep=False)]\n",
        "print(dups_calls_full.head(10))\n",
        "\n",
        "# what if calls happened simultaneously from two devices?\n",
        "# check overlapping calls per manager\n",
        "group_cols = [\"OWNER\", \"Call Date\", \"Call Time\"]\n",
        "data_calls[\"calls_at_same_time\"] = data_calls.groupby(group_cols)[\"Call Time\"].transform(\n",
        "    \"count\"\n",
        ")\n",
        "\n",
        "parallel_calls = (\n",
        "    data_calls[data_calls[\"calls_at_same_time\"] > 1]\n",
        "    .sort_values(group_cols + [\"Duration\"])\n",
        "    .query(\"calls_at_same_time > 1\")   # keep only simultaneous\n",
        ")\n",
        "\n",
        "print(\"Cases where a manager had several calls at the same time:\")\n",
        "print(parallel_calls.head(20))\n",
        "\n",
        "# if a call has an exact duplicate (everything same except Id and duration) —\n",
        "# keep only one, with duration closest to that manager's median\n",
        "\n",
        "# 0) make sure duration is numeric\n",
        "data_calls[\"Duration\"] = pd.to_numeric(data_calls[\"Duration\"], errors=\"coerce\")\n",
        "\n",
        "# 1) duration medians per manager\n",
        "medians_by_owner = data_calls.groupby(\"OWNER\")[\"Duration\"].median()\n",
        "\n",
        "# 2) grouping columns: everything except Id and duration\n",
        "group_cols = [c for c in data_calls.columns if c not in [\"Id\", \"Duration\"]]\n",
        "\n",
        "# 3) function to choose row index whose duration is closest to manager median\n",
        "def pick_best_index(group: pd.DataFrame) -> int:\n",
        "    owner = group[\"OWNER\"].iloc[0]\n",
        "    median_owner = medians_by_owner.get(owner, np.nan)\n",
        "\n",
        "    # if manager median is undefined (all NaN), use group median\n",
        "    if pd.isna(median_owner):\n",
        "        median_group = group[\"Duration\"].median()\n",
        "    else:\n",
        "        median_group = median_owner\n",
        "\n",
        "    # if that also fails (all NaN), return first row in group\n",
        "    if pd.isna(median_group):\n",
        "        return group.index[0]\n",
        "\n",
        "    # distance to median (replace NaN with large number to avoid choosing them)\n",
        "    distances = (group[\"Duration\"] - median_group).abs().fillna(1e9)\n",
        "\n",
        "    # index of row with minimal deviation\n",
        "    best_idx = distances.idxmin()\n",
        "    return best_idx\n",
        "\n",
        "\n",
        "# 4) pick best index from each group\n",
        "best_indices = []\n",
        "for _, g in data_calls.groupby(group_cols):\n",
        "    if len(g) == 1:\n",
        "        best_indices.append(g.index[0])\n",
        "    else:\n",
        "        best_indices.append(pick_best_index(g))\n",
        "\n",
        "best_indices = pd.Index(best_indices)\n",
        "\n",
        "# if Series with MultiIndex is returned — take index values\n",
        "best_indices = best_indices.unique()\n",
        "\n",
        "# 5) build deduplicated dataframe\n",
        "before = len(data_calls)\n",
        "data_calls_dedup = data_calls.loc[best_indices].copy()\n",
        "after = len(data_calls_dedup)\n",
        "\n",
        "print(f\"Before: {before} rows\")\n",
        "print(f\"After deduplication: {after} rows\")\n",
        "print(f\"Removed overlapping calls: {before - after}\")\n",
        "\n",
        "sns.boxplot(x=data_calls_dedup[\"Duration\"])\n",
        "plt.title(\"Call duration boxplot (seconds)\")\n",
        "plt.xlabel(\"Duration (sec)\")\n",
        "plt.show()\n",
        "\n",
        "# see how to bin calls by duration\n",
        "duration_sec = pd.to_numeric(data_calls_dedup[\"Duration\"], errors=\"coerce\")\n",
        "\n",
        "# 2) create duration-in-minutes column\n",
        "duration_min = duration_sec / 60.0\n",
        "\n",
        "# 3) histogram + category boundaries\n",
        "plt.hist(duration_min, bins=50)\n",
        "for x in [2, 5, 15]:\n",
        "    plt.axvline(x, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "plt.xlim(0, 60)  # to see the main mass\n",
        "plt.title(\"Distribution of call duration (minutes)\")\n",
        "plt.xlabel(\"Call duration, min\")\n",
        "plt.ylabel(\"Number of calls\")\n",
        "plt.show()\n",
        "\n",
        "# our boundaries\n",
        "labels = [\"short <2\", \"medium 2–5\", \"long 5–15\", \"super long >15\"]\n",
        "\n",
        "\n",
        "def categorize_duration(x: float) -> str:\n",
        "    if x < 2:\n",
        "        return \"short (<2)\"\n",
        "    if x < 5:\n",
        "        return \"medium (2–5)\"\n",
        "    if x < 15:\n",
        "        return \"long (5–15)\"\n",
        "    return \"super long (>15)\"\n",
        "\n",
        "\n",
        "order = [\"short (<2)\", \"medium (2–5)\", \"long (5–15)\", \"super long (>15)\"]\n",
        "\n",
        "data_calls_dedup[\"Duration_min\"] = duration_min\n",
        "data_calls_dedup[\"Duration_cat\"] = data_calls_dedup[\"Duration_min\"].apply(\n",
        "    categorize_duration\n",
        ")\n",
        "\n",
        "sns.boxplot(\n",
        "    x=\"Duration_min\",\n",
        "    y=\"Duration_cat\",\n",
        "    data=data_calls_dedup,\n",
        "    order=order,\n",
        "    showfliers=True   # show outliers as dots\n",
        ")\n",
        "plt.title(\"Call duration boxplot by category\")\n",
        "plt.xlabel(\"Call duration, min\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.show()\n",
        "\n",
        "# compute outlier bounds for each category\n",
        "bounds = {}\n",
        "for cat in order:\n",
        "    vals = data_calls_dedup.loc[\n",
        "        data_calls_dedup[\"Duration_cat\"] == cat, \"Duration_min\"\n",
        "    ].dropna()\n",
        "    if len(vals) == 0:\n",
        "        bounds[cat] = (np.nan, np.nan)\n",
        "    else:\n",
        "        q1 = vals.quantile(0.25)\n",
        "        q3 = vals.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        low = q1 - 1.5 * iqr\n",
        "        high = q3 + 1.5 * iqr\n",
        "        bounds[cat] = (low, high)\n",
        "\n",
        "print(\"Statistics and outlier bounds:\")\n",
        "for cat, (low, high) in bounds.items():\n",
        "    print(f\"{cat}: low={low}, high={high}\")\n",
        "\n",
        "# convert to numbers and minutes\n",
        "bounds_min = {\n",
        "    cat: (float(low), float(high))\n",
        "    for cat, (low, high) in bounds.items()\n",
        "    if not (np.isnan(low) or np.isnan(high))\n",
        "}\n",
        "\n",
        "# categories (in minutes)\n",
        "def is_valid_duration(row):\n",
        "    cat = row[\"Duration_cat\"]\n",
        "    val = row[\"Duration_min\"]\n",
        "    if cat not in bounds_min:\n",
        "        return False\n",
        "    low, high = bounds_min[cat]\n",
        "    return low <= val <= high\n",
        "\n",
        "\n",
        "# propagate bounds to rows\n",
        "data_calls_dedup[\"is_valid\"] = data_calls_dedup.apply(is_valid_duration, axis=1)\n",
        "\n",
        "# validity: within bounds and not NaN\n",
        "before = len(data_calls_dedup)\n",
        "data_calls_clean = data_calls_dedup[\n",
        "    data_calls_dedup[\"is_valid\"] & data_calls_dedup[\"Duration_min\"].notna()\n",
        "]\n",
        "after = len(data_calls_clean)\n",
        "\n",
        "print(f\"Before cleaning: {before}\")\n",
        "print(f\"After removing outliers: {after}\")\n",
        "print(f\"Removed: {before - after}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "sns.boxplot(\n",
        "    x=\"Duration_min\",\n",
        "    y=\"Duration_cat\",\n",
        "    data=data_calls_clean,\n",
        "    order=order,\n",
        "    ax=ax,\n",
        ")\n",
        "ax.set_title(\"Call duration boxplot by category\")\n",
        "ax.set_xlabel(\"Call duration, min\")\n",
        "ax.set_ylabel(\"Category\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# save cleaned dataframe\n",
        "data_calls_clean.to_excel(\"data_calls_clean.xlsx\", index=False)\n",
        "\n",
        "print(\"File saved successfully: data_calls_clean.xlsx\")\n",
        "\n",
        "\"\"\"## Contacts Cleaning\"\"\"\n",
        "\n",
        "data_contacts = pd.read_excel(\"DWW Contacts Done.xlsx\")\n",
        "print(data_contacts.head())\n",
        "print(data_contacts.info())\n",
        "\n",
        "# convert Created Time and Modified Time to datetime\n",
        "datetime_cols = [\"Created Time\", \"Modified Time\"]\n",
        "for col in datetime_cols:\n",
        "    if col in data_contacts.columns:\n",
        "        data_contacts[col] = pd.to_datetime(\n",
        "            data_contacts[col], errors=\"coerce\"\n",
        "        )\n",
        "\n",
        "# create new columns\n",
        "for base_col in datetime_cols:\n",
        "    if base_col in data_contacts.columns:\n",
        "        data_contacts[f\"{base_col} Date\"] = data_contacts[base_col].dt.date\n",
        "        data_contacts[f\"{base_col} Time\"] = data_contacts[base_col].dt.time\n",
        "\n",
        "# check\n",
        "print(\n",
        "    data_contacts[\n",
        "        [\"Created Time\", \"Created Time Date\", \"Created Time Time\"]\n",
        "    ].head(10)\n",
        ")\n",
        "\n",
        "# transform Id: cast to string and prefix with \"A\"\n",
        "if \"CONTACTID\" in data_contacts.columns:\n",
        "    data_contacts[\"CONTACTID\"] = data_contacts[\"CONTACTID\"].astype(str)\n",
        "    data_contacts[\"CONTACTID\"] = data_contacts[\"CONTACTID\"].apply(\n",
        "        lambda x: \"A\" + x if x.strip() != \"\" else x\n",
        "    )\n",
        "\n",
        "# find all full duplicates (all columns equal)\n",
        "dups_contacts = data_contacts[data_contacts.duplicated(keep=False)]\n",
        "\n",
        "print(f\"Total rows in table: {len(data_contacts)}\")\n",
        "print(\n",
        "    f\"Number of rows that are part of full duplicates: {len(dups_contacts)}\"\n",
        ")\n",
        "print(\n",
        "    f\"Number of unique full duplicates: {dups_contacts.duplicated().sum()}\"\n",
        ")\n",
        "\n",
        "# show several examples of duplicates\n",
        "print(dups_contacts.head(10))\n",
        "\n",
        "# number of NaN per column\n",
        "nan_percent_contacts = (\n",
        "    data_contacts.isna().sum() / len(data_contacts) * 100\n",
        ").round(2)\n",
        "print(\"Missing values in data_contacts:\")\n",
        "print(nan_percent_contacts)\n",
        "\n",
        "# save cleaned dataframe\n",
        "data_contacts.to_excel(\"data_contacts_clean.xlsx\", index=False)\n",
        "\n",
        "print(\"File saved successfully: data_contacts_clean.xlsx\")\n",
        "\n",
        "# since Google Sheets refuses to read columns in correct type, use a workaround\n",
        "def add_prefix_A(col: pd.Series) -> pd.Series:\n",
        "    col = col.astype(str).fillna(\"\")                # replace NaN with empty string\n",
        "    col = col.astype(str)               # to string\n",
        "    col = col.str.replace(\".\", \"\", regex=False)  # remove dots\n",
        "    return col.where(col.eq(\"\"), \"A\" + col)   # keep empty as \"\", prepend \"A\" to others\n",
        "\n",
        "\n",
        "\"\"\"## Final assembly into a single Excel file\"\"\"\n",
        "\n",
        "def safe_read_excel(path: str, var_name: str, fallback_file: str):\n",
        "    \"\"\"\n",
        "    Try to read from existing dataframe (if defined), otherwise from file.\n",
        "    \"\"\"\n",
        "    if var_name in globals():\n",
        "        return globals()[var_name]\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            return pd.read_excel(path)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to read {p}: {e}\")\n",
        "    raise RuntimeError(f\"Neither dataframe '{var_name}' nor file '{fallback_file}' found.\")\n",
        "\n",
        "\n",
        "data_spend_final = safe_read_excel(\"data_spend_clean.xlsx\", \"data_spend\", \"Spend (Done).xlsx\")\n",
        "data_deals_final = safe_read_excel(\"data_deals_clean.xlsx\", \"data_deals\", \"DWW Deals Done.xlsx\")\n",
        "data_calls_final = safe_read_excel(\"data_calls_clean.xlsx\", \"data_calls_clean\", \"DWW Calls Done.xlsx\")\n",
        "data_contacts_final = safe_read_excel(\"data_contacts_clean.xlsx\", \"data_contacts\", \"DWW Contacts Done.xlsx\")\n",
        "\n",
        "with pd.ExcelWriter(\"projekt.xlsx\") as writer:\n",
        "    data_spend_final.to_excel(writer, sheet_name=\"spend\", index=False)\n",
        "    data_deals_final.to_excel(writer, sheet_name=\"deals\", index=False)\n",
        "    data_calls_final.to_excel(writer, sheet_name=\"calls\", index=False)\n",
        "    data_contacts_final.to_excel(writer, sheet_name=\"contacts\", index=False)\n",
        "\n",
        "print(\"File saved successfully: projekt.xlsx (sheets: spend, deals, calls, contacts)\")"
      ]
    }
  ]
}